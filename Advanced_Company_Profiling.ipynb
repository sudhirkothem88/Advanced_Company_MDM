{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b0131c5",
   "metadata": {},
   "source": [
    "\n",
    "# Advanced Company Profiling & Data Validation (UK Companies)\n",
    "\n",
    "**Objective:** Profile, cleanse, deduplicate, match and enrich UK company contact records using the **Companies House REST API**, then produce clear **reports and charts**.  \n",
    "> Put your `Company.csv` in this folder before running the notebook.\n",
    "\n",
    "**Pipeline**\n",
    "1. Data Collection (load `Company.csv`)\n",
    "2. Profiling\n",
    "3. Cleansing\n",
    "4. Deduplication\n",
    "5. Matching (Companies House REST API)\n",
    "6. Enrichment\n",
    "7. Reporting & visualisation\n",
    "8. Packaging / outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977493b6",
   "metadata": {},
   "source": [
    "## 0. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdf0840",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install deps if needed (uncomment for first run)\n",
    "# %pip install -r requirements.txt\n",
    "\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz import fuzz, process\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional: read API key from .env if present\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "DATA_PATH = Path(\"Company.csv\")  # Ensure this file exists in the project root\n",
    "OUTPUT_DIR = Path(\"outputs\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Companies House API configuration\n",
    "# Obtain an API key: https://developer.company-information.service.gov.uk/get-started\n",
    "# Auth: Basic Auth with API key as username, empty password.\n",
    "COMPANIES_HOUSE_API_KEY = os.getenv(\"COMPANIES_HOUSE_API_KEY\", \"\")\n",
    "BASE_URL = \"https://api.company-information.service.gov.uk\"  # Official base\n",
    "\n",
    "session = requests.Session()\n",
    "session.auth = (COMPANIES_HOUSE_API_KEY, \"\")\n",
    "\n",
    "print(\"API key configured:\", bool(COMPANIES_HOUSE_API_KEY))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751fa4ed",
   "metadata": {},
   "source": [
    "## 1. Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7351f9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load input CSV\n",
    "assert DATA_PATH.exists(), \"Company.csv not found. Place it next to this notebook.\"\n",
    "df_raw = pd.read_csv(DATA_PATH)\n",
    "print(\"Rows:\", len(df_raw))\n",
    "display(df_raw.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b32e8c9",
   "metadata": {},
   "source": [
    "## 2. Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb4267",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",   
    "import pandas as pd",

    "def profile_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    summary = []\n",
    "    for col in df.columns:\n",
    "        s = df[col]\n",
    "        summary.append({\n",
    "            \"column\": col,\n",
    "            \"dtype\": s.dtype.name,\n",
    "            \"non_null\": int(s.notna().sum()),\n",
    "            \"nulls\": int(s.isna().sum()),\n",
    "            \"unique\": int(s.nunique(dropna=True)),\n",
    "            \"sample_values\": list(s.dropna().astype(str).head(3))\n",
    "        })\n",
    "    return pd.DataFrame(summary)\n",
    "\n",
    "profile = profile_dataframe(df_raw)\n",
    "display(profile)\n",
    "\n",
    "# Basic null matrix (% of nulls)\n",
    "null_pct = df_raw.isna().mean().sort_values(ascending=False)\n",
    "display(null_pct.to_frame(\"null_pct\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10530809",
   "metadata": {},
   "source": [
    "## 3. Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a8a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "LEGAL_SUFFIX_MAP = {\n",
    "    \"LTD\": \"LIMITED\",\n",
    "    \"LTD.\": \"LIMITED\",\n",
    "    \"LIMITED\": \"LIMITED\",\n",
    "    \"PLC\": \"PLC\",\n",
    "    \"LLP\": \"LLP\",\n",
    "    \"L.L.P\": \"LLP\",\n",
    "}\n",
    "\n",
    "def normalize_whitespace(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def strip_punct(s: str) -> str:\n",
    "    return s.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "def normalize_company_name(name: str) -> str:\n",
    "    if not isinstance(name, str) or not name.strip():\n",
    "        return \"\"\n",
    "    # Unicode normalize\n",
    "    name = unicodedata.normalize(\"NFKC\", name)\n",
    "    # Upper-case\n",
    "    name = name.upper()\n",
    "    # Standardize legal suffixes\n",
    "    tokens = normalize_whitespace(strip_punct(name)).split(\" \")\n",
    "    tokens = [t for t in tokens if t]\n",
    "    # Map suffix if present\n",
    "    if tokens and tokens[-1] in LEGAL_SUFFIX_MAP:\n",
    "        tokens[-1] = LEGAL_SUFFIX_MAP[tokens[-1]]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "def normalize_postcode(pc: str) -> str:\n",
    "    if not isinstance(pc, str):\n",
    "        return \"\"\n",
    "    return normalize_whitespace(pc.upper())\n",
    "\n",
    "def normalize_email(email: str) -> str:\n",
    "    if not isinstance(email, str):\n",
    "        return \"\"\n",
    "    return email.strip().lower()\n",
    "\n",
    "def cleanse(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    # Heuristic column detection\n",
    "    name_col = None\n",
    "    for cand in [\"company_name\",\"CompanyName\",\"name\",\"Name\",\"Company\",\"Organisation\",\"organisation\"]:\n",
    "        if cand in out.columns:\n",
    "            name_col = cand\n",
    "            break\n",
    "    if name_col is None:\n",
    "        # Fall back to first column\n",
    "        name_col = out.columns[0]\n",
    "\n",
    "    if \"postcode\" in out.columns:\n",
    "        out[\"postcode_norm\"] = out[\"postcode\"].apply(normalize_postcode)\n",
    "    elif \"Postcode\" in out.columns:\n",
    "        out[\"postcode_norm\"] = out[\"Postcode\"].apply(normalize_postcode)\n",
    "    else:\n",
    "        out[\"postcode_norm\"] = \"\"\n",
    "\n",
    "    # Email normalization if present\n",
    "    if \"email\" in out.columns:\n",
    "        out[\"email_norm\"] = out[\"email\"].apply(normalize_email)\n",
    "    else:\n",
    "        out[\"email_norm\"] = \"\"\n",
    "\n",
    "    out[\"company_name_norm\"] = out[name_col].astype(str).apply(normalize_company_name)\n",
    "    # Blocking key for dedup (tune as needed)\n",
    "    out[\"block_key\"] = out[\"company_name_norm\"].str.replace(\" \", \"\") + \"|\" + out[\"postcode_norm\"]\n",
    "    return out\n",
    "\n",
    "df_clean = cleanse(df_raw)\n",
    "display(df_clean.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1839080f",
   "metadata": {},
   "source": [
    "## 4. Deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42906e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Rule-based: drop exact duplicates on normalized keys first\n",
    "dedup_cols = [\"company_name_norm\",\"postcode_norm\",\"email_norm\"]\n",
    "before = len(df_clean)\n",
    "df_dedup = df_clean.drop_duplicates(subset=dedup_cols, keep=\"first\").copy()\n",
    "print(f\"Exact dedup removed {before - len(df_dedup)} rows\")\n",
    "\n",
    "# Fuzzy dedup within blocks using RapidFuzz\n",
    "def fuzzy_cluster_block(block_df: pd.DataFrame, name_col=\"company_name_norm\", threshold=95):\n",
    "    # Greedy clustering by name similarity\n",
    "    taken = set()\n",
    "    clusters = []\n",
    "    names = block_df[name_col].tolist()\n",
    "    idxs = block_df.index.tolist()\n",
    "    for i, idx in enumerate(idxs):\n",
    "        if idx in taken:\n",
    "            continue\n",
    "        rep = names[i]\n",
    "        cluster = [idx]\n",
    "        taken.add(idx)\n",
    "        # compare with remaining in block\n",
    "        for j in range(i+1, len(idxs)):\n",
    "            jidx = idxs[j]\n",
    "            if jidx in taken: \n",
    "                continue\n",
    "            score = fuzz.token_set_ratio(rep, names[j])\n",
    "            if score >= threshold:\n",
    "                cluster.append(jidx)\n",
    "                taken.add(jidx)\n",
    "        clusters.append(cluster)\n",
    "    return clusters\n",
    "\n",
    "# Apply fuzzy clustering per block\n",
    "clusters = []\n",
    "for block_val, block_df in df_dedup.groupby(\"block_key\"):\n",
    "    if len(block_df) == 1:\n",
    "        clusters.append([block_df.index[0]])\n",
    "    else:\n",
    "        clusters.extend(fuzzy_cluster_block(block_df, threshold=93))\n",
    "\n",
    "# Choose first record as canonical in each cluster\n",
    "canonical_idx = [c[0] for c in clusters]\n",
    "df_canonical = df_dedup.loc[canonical_idx].copy().reset_index(drop=True)\n",
    "print(\"Canonical records:\", len(df_canonical), \"from\", len(df_dedup))\n",
    "\n",
    "# Save mapping of original -> canonical if needed\n",
    "cluster_map = {}\n",
    "for cid, cl in enumerate(clusters):\n",
    "    for idx in cl:\n",
    "        cluster_map[idx] = cid\n",
    "df_dedup[\"cluster_id\"] = df_dedup.index.map(cluster_map)\n",
    "df_dedup.to_csv(OUTPUT_DIR / \"dedup_clusters.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92702e35",
   "metadata": {},
   "source": [
    "## 5. Matching (Companies House) & Enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb382d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from urllib.parse import quote\n",
    "\n",
    "def ch_search_company(query: str, items_per_page: int = 5):\n",
    "    if not COMPANIES_HOUSE_API_KEY:\n",
    "        raise RuntimeError(\"Set COMPANIES_HOUSE_API_KEY env var before calling Companies House API.\")\n",
    "    url = f\"{BASE_URL}/search/companies?q={quote(query)}&items_per_page={items_per_page}\"\n",
    "    r = session.get(url, timeout=20)\n",
    "    if r.status_code == 429:\n",
    "        # Rate-limited; caller can retry with backoff\n",
    "        return {\"rate_limited\": True, \"status_code\": r.status_code}\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def best_match_for_name(name: str):\n",
    "    data = ch_search_company(name, items_per_page=10)\n",
    "    if isinstance(data, dict) and data.get(\"rate_limited\"):\n",
    "        return None\n",
    "    items = data.get(\"items\", [])\n",
    "    if not items:\n",
    "        return None\n",
    "    # Score candidates by fuzzy similarity on title\n",
    "    scored = []\n",
    "    for it in items:\n",
    "        title = it.get(\"title\",\"\") or it.get(\"company_name\",\"\")\n",
    "        score = fuzz.token_set_ratio(name.upper(), (title or \"\").upper())\n",
    "        scored.append((score, it))\n",
    "    scored.sort(reverse=True, key=lambda x: x[0])\n",
    "    return scored[0][1] if scored else None\n",
    "\n",
    "enrich_cols = [\n",
    "    \"company_number\",\"company_status\",\"company_type\",\"date_of_creation\",\n",
    "    \"address_snippet\",\"kind\",\"title\",\"links\"\n",
    "]\n",
    "\n",
    "matches = []\n",
    "for name in tqdm(df_canonical[\"company_name_norm\"], desc=\"Matching Companies House\"):\n",
    "    try:\n",
    "        m = best_match_for_name(name)\n",
    "    except Exception as e:\n",
    "        m = None\n",
    "    matches.append(m)\n",
    "\n",
    "df_matches = pd.json_normalize(matches)[enrich_cols] if any(matches) else pd.DataFrame(columns=enrich_cols)\n",
    "df_enriched = pd.concat([df_canonical.reset_index(drop=True), df_matches.reset_index(drop=True)], axis=1)\n",
    "display(df_enriched.head())\n",
    "\n",
    "df_enriched.to_csv(OUTPUT_DIR / \"enriched_companies.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2850862e",
   "metadata": {},
   "source": [
    "## 6. Reporting & Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02c25c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# KPIs\n",
    "kpis = {\n",
    "    \"input_rows\": len(df_raw),\n",
    "    \"after_exact_dedup\": len(df_dedup),\n",
    "    \"canonical_records\": len(df_canonical),\n",
    "    \"enriched_records\": int(df_enriched[\"company_number\"].notna().sum()) if \"company_number\" in df_enriched else 0,\n",
    "    \"match_rate_pct\": round(100.0 * (df_enriched[\"company_number\"].notna().mean() if \"company_number\" in df_enriched else 0.0), 2)\n",
    "}\n",
    "print(kpis)\n",
    "\n",
    "# Bar chart: Status distribution (if present)\n",
    "if \"company_status\" in df_enriched:\n",
    "    status_counts = df_enriched[\"company_status\"].fillna(\"unmatched\").value_counts().sort_values(ascending=False)\n",
    "    plt.figure()\n",
    "    status_counts.plot(kind=\"bar\", title=\"Company status distribution\")\n",
    "    plt.xlabel(\"status\"); plt.ylabel(\"count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddfa70c",
   "metadata": {},
   "source": [
    "## 7. Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b4655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save final outputs\n",
    "df_profile = profile_dataframe(df_raw)\n",
    "df_profile.to_csv(OUTPUT_DIR / \"profile.csv\", index=False)\n",
    "\n",
    "df_enriched.to_parquet(OUTPUT_DIR / \"enriched_companies.parquet\", index=False)\n",
    "print(\"Files written to:\", OUTPUT_DIR.resolve())\n",
    "list(OUTPUT_DIR.glob(\"*\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ec916c",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Notes & Next Steps\n",
    "- Tune **normalisation** and **blocking** to fit your data (e.g., include city or first line of address).\n",
    "- Adjust **fuzzy thresholds**; lower threshold finds more matches but risks false positives.\n",
    "- Use **exponential backoff** on HTTP 429 (rate limiting: ~600 requests / 5 minutes).\n",
    "- Consider caching API responses locally to avoid re-calling the same names.\n",
    "- For high volumes, batch processing and resume checkpoints are recommended.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
