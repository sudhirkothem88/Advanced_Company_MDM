{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "977493b6",
   "metadata": {},
   "source": [
    "## 0. Setup & Configuration\n",
    "Responsible for imports, paths, session auth, and environment loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e611eb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install deps if needed (uncomment for first run)\n",
    "# %pip install -r requirements.txt\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz import fuzz\n",
    "import matplotlib.pyplot as plt\n",
    "import unicodedata, string\n",
    "from urllib.parse import quote\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "DATA_PATH = Path(\"Company.csv\")\n",
    "OUTPUT_DIR = Path(\"outputs\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Secrets\n",
    "load_dotenv()\n",
    "COMPANIES_HOUSE_API_KEY = os.getenv(\"COMPANIES_HOUSE_API_KEY\", \"\")\n",
    "BASE_URL = \"https://api.company-information.service.gov.uk\"\n",
    "\n",
    "# Session\n",
    "session = requests.Session()\n",
    "if COMPANIES_HOUSE_API_KEY:\n",
    "    session.auth = (COMPANIES_HOUSE_API_KEY, \"\")\n",
    "\n",
    "print(\"API key present:\", bool(COMPANIES_HOUSE_API_KEY))\n",
    "print(\"Data path exists:\", DATA_PATH.exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751fa4ed",
   "metadata": {},
   "source": [
    "## 1. Data Collection\n",
    "Load the provided `Company.csv` exactly as-is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7351f9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load input CSV\n",
    "assert DATA_PATH.exists(), \"Company.csv\"\n",
    "df_raw = pd.read_csv(DATA_PATH)\n",
    "print(\"Rows:\", len(df_raw))\n",
    "display(df_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7392bfe0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b32e8c9",
   "metadata": {},
   "source": [
    "## 2. Profiling\n",
    "Lightweight schema/quality profile to understand columns, nulls, and uniqueness. These checks are **non-destructive**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcb4267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple schema/quality profile\n",
    "def profile_dataframe(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for col in df.columns:\n",
    "        s = df[col]\n",
    "        rows.append({\n",
    "            \"column\": col,\n",
    "            \"dtype\": s.dtype.name,\n",
    "            \"nulls\": int(s.isna().sum()),\n",
    "            \"non_null\": int(s.notna().sum()),\n",
    "            \"unique\": int(s.nunique(dropna=True)),\n",
    "            \"sample_values\": list(s.dropna().astype(str).head(3))\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "profile = profile_dataframe(df_raw)\n",
    "display(profile)\n",
    "\n",
    "null_pct = df_raw.isna().mean().sort_values(ascending=False)\n",
    "display(null_pct.to_frame(\"null_pct\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016801aa",
   "metadata": {},
   "source": [
    "## 3. Cleansing (Non-destructive normalization)\n",
    "Create derived, normalized fields while preserving originals. Goals:\n",
    "\n",
    "- Handle whitespace, casing, punctuation, and accents in names\n",
    "- Normalize UK postcodes to a consistent format\n",
    "- Normalize emails to lowercase\n",
    "- Build a `block_key` for within-block fuzzy deduplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a8a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, unicodedata, string\n",
    "\n",
    "COMPANY_SUFFIXES = {\n",
    "    \"LTD\": \"LIMITED\", \"LTD.\": \"LIMITED\",\n",
    "    \"LIMITED\": \"LIMITED\", \"PLC\": \"PLC\",\n",
    "    \"LLP\": \"LLP\", \"L.L.P\": \"LLP\"\n",
    "}\n",
    "\n",
    "def normalize_whitespace(s):\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def strip_punct(s):\n",
    "    return \"\".join(ch for ch in s if ch not in set(string.punctuation))\n",
    "\n",
    "def normalize_company_name(s):\n",
    "    s = normalize_whitespace(s).upper()\n",
    "    # remove accents\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    s = \"\".join([c for c in s if not unicodedata.combining(c)])\n",
    "    # Normalize common suffixes\n",
    "    toks = s.split(\" \")\n",
    "    toks = [COMPANY_SUFFIXES.get(t, t) for t in toks]\n",
    "    s = \" \".join(toks)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def normalize_postcode(s):\n",
    "    s = normalize_whitespace(str(s))\n",
    "    s = s.upper().replace(\" \", \"\")\n",
    "    if len(s) > 3:\n",
    "        s = s[:-3] + \" \" + s[-3:]\n",
    "    return s.strip()\n",
    "\n",
    "def normalize_email(s):\n",
    "    s = normalize_whitespace(str(s)).lower()\n",
    "    return s\n",
    "\n",
    "def cleanse(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "\n",
    "    # company name\n",
    "    name_col = next((c for c in out.columns if \"company\" in c.lower() and \"name\" in c.lower()), None)\n",
    "    if name_col is None:\n",
    "        out[\"company_name_norm\"] = \"\"\n",
    "    else:\n",
    "        out[\"company_name_norm\"] = out[name_col].astype(str).apply(normalize_company_name)\n",
    "\n",
    "    # postcode\n",
    "    postcode_col = next((c for c in out.columns if \"postcode\" in c.lower()), None)\n",
    "    if postcode_col is None:\n",
    "        out[\"postcode_norm\"] = \"\"\n",
    "    else:\n",
    "        out[\"postcode_norm\"] = out[postcode_col].astype(str).apply(normalize_postcode)\n",
    "\n",
    "    # email (optional)\n",
    "    email_col = next((c for c in out.columns if \"email\" in c.lower()), None)\n",
    "    if email_col is None:\n",
    "        out[\"email_norm\"] = \"\"\n",
    "    else:\n",
    "        out[\"email_norm\"] = out[email_col].astype(str).apply(normalize_email)\n",
    "\n",
    "    # block key\n",
    "    out[\"block_key\"] = out[\"company_name_norm\"].str.replace(\" \", \"\", regex=False) + \"|\" + out[\"postcode_norm\"].fillna(\"\")\n",
    "    return out\n",
    "\n",
    "df_clean = cleanse(df_raw)\n",
    "display(df_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1839080f",
   "metadata": {},
   "source": [
    "## 4. Deduplication\n",
    "Two steps:\n",
    "\n",
    "1. **Exact**: drop duplicates on normalized columns  \n",
    "2. **Fuzzy within block**: cluster near-duplicates using `token_sort_ratio` and take a canonical record per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42906e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact dedup on normalized keys\n",
    "dedup_cols = [\"company_name_norm\",\"postcode_norm\",\"email_norm\"]\n",
    "dedup_cols = [c for c in dedup_cols if c in df_clean.columns]\n",
    "before = len(df_clean)\n",
    "df_dedup = df_clean.drop_duplicates(subset=dedup_cols, keep=\"first\").copy()\n",
    "print(f\"Exact dedup removed {before - len(df_dedup)} rows\")\n",
    "\n",
    "# Fuzzy clustering within blocks\n",
    "def fuzzy_cluster_block(block_df: pd.DataFrame, name_col=\"company_name_norm\", threshold=95):\n",
    "    taken = set()\n",
    "    clusters = []\n",
    "    names = block_df[name_col].tolist()\n",
    "    idxs = block_df.index.tolist()\n",
    "    for i, idx in enumerate(idxs):\n",
    "        if idx in taken:\n",
    "            continue\n",
    "        base = names[i]\n",
    "        cl = [idx]\n",
    "        taken.add(idx)\n",
    "        for j in range(i+1, len(idxs)):\n",
    "            jdx = idxs[j]\n",
    "            if jdx in taken:\n",
    "                continue\n",
    "            score = fuzz.token_sort_ratio(base, names[j])\n",
    "            if score >= threshold:\n",
    "                cl.append(jdx)\n",
    "                taken.add(jdx)\n",
    "        clusters.append(cl)\n",
    "    return clusters\n",
    "\n",
    "clusters = []\n",
    "for key, block_df in df_dedup.groupby(\"block_key\"):\n",
    "    if len(block_df) == 1:\n",
    "        clusters.append([block_df.index[0]])\n",
    "    else:\n",
    "        clusters.extend(fuzzy_cluster_block(block_df, threshold=93))\n",
    "\n",
    "canonical_idx = [c[0] for c in clusters]\n",
    "df_canonical = df_dedup.loc[canonical_idx].copy().reset_index(drop=True)\n",
    "print(\"Canonical records:\", len(df_canonical), \"from\", len(df_dedup))\n",
    "\n",
    "# optional mapping\n",
    "cluster_map = {}\n",
    "for cid, cl in enumerate(clusters):\n",
    "    for idx in cl:\n",
    "        cluster_map[idx] = cid\n",
    "df_dedup = df_dedup.copy()\n",
    "df_dedup[\"cluster_id\"] = df_dedup.index.map(cluster_map)\n",
    "df_dedup.to_csv(OUTPUT_DIR / \"dedup_clusters.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06402c5",
   "metadata": {},
   "source": [
    "### Performance & rate limits\n",
    "API calls dominate runtime. Expect variable latency and occasional 429s. The small retry/backoff in code keeps the run going without manual intervention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448fad6e",
   "metadata": {},
   "source": [
    "## 5. Matching (Companies House) & Enrichment\n",
    "**Rationale**: Local text similarity recovers probable matches. We then call Companies House to retrieve authoritative IDs and status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb382d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------\n",
    "# Companies House search function\n",
    "# -----------------------------------\n",
    "def ch_search_company(query: str, items_per_page: int = 5, retries: int = 2, backoff: float = 1.5):\n",
    "    if not COMPANIES_HOUSE_API_KEY:\n",
    "        raise RuntimeError(\"Set COMPANIES_HOUSE_API_KEY in .env before calling Companies House API.\")\n",
    "\n",
    "    url = f\"{BASE_URL}/search/companies?q={quote(query)}&items_per_page={items_per_page}\"\n",
    "\n",
    "    for attempt in range(retries + 1):\n",
    "        try:\n",
    "            r = session.get(url, timeout=20)\n",
    "        except Exception as e:\n",
    "            print(f\" Request failed for '{query}': {e}\")\n",
    "            return {\"error\": True, \"status_code\": None}\n",
    "\n",
    "        # Handle rate limits with retry backoff\n",
    "        if r.status_code == 429 and attempt < retries:\n",
    "            time.sleep(backoff * (attempt + 1))\n",
    "            continue\n",
    "\n",
    "        # Status code error → mark as failed\n",
    "        if r.status_code >= 400:\n",
    "            return {\"error\": True, \"status_code\": r.status_code}\n",
    "\n",
    "        try:\n",
    "            return r.json()\n",
    "        except Exception:\n",
    "            return {\"error\": True, \"status_code\": r.status_code}\n",
    "\n",
    "    return {\"error\": True, \"status_code\": 429}\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "#  fuzzy match selector\n",
    "# -----------------------------------\n",
    "def best_match_for_name(name: str):\n",
    "    data = ch_search_company(name, items_per_page=10)\n",
    "\n",
    "    # FIX 1: Skip if API returned error or malformed response\n",
    "    if not isinstance(data, dict) or data.get(\"error\"):\n",
    "        return None\n",
    "\n",
    "    items = data.get(\"items\", [])\n",
    "    if not items:\n",
    "        return None\n",
    "\n",
    "    # Compute fuzzy match scores for all candidates\n",
    "    candidates = []\n",
    "    for it in items:\n",
    "        reg_name = it.get(\"title\") or it.get(\"company_name\") or \"\"\n",
    "        score = fuzz.token_set_ratio(name.upper(), reg_name.upper())\n",
    "        candidates.append((score, it))\n",
    "\n",
    "    if not candidates:\n",
    "        return None\n",
    "\n",
    "    # Sort by descending match score\n",
    "    candidates.sort(reverse=True, key=lambda x: x[0])\n",
    "    best_score, best_item = candidates[0]\n",
    "\n",
    "    #  FIX 2: Safely flatten keys to avoid missing-field errors\n",
    "    out = {\n",
    "        \"ch_best_title\": best_item.get(\"title\") or best_item.get(\"company_name\"),\n",
    "        \"ch_best_company_number\": best_item.get(\"company_number\", None),\n",
    "        \"ch_best_company_status\": best_item.get(\"company_status\", None),\n",
    "        \"ch_best_address_snippet\": best_item.get(\"address_snippet\", None),\n",
    "        \"ch_best_date_of_creation\": best_item.get(\"date_of_creation\", None),\n",
    "        \"ch_best_score\": best_score,\n",
    "    }\n",
    "    return out\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# Enrichment pipeline\n",
    "# -----------------------------------\n",
    "enrich_cols = [\n",
    "    \"ch_best_title\",\n",
    "    \"ch_best_company_number\",\n",
    "    \"ch_best_company_status\",\n",
    "    \"ch_best_address_snippet\",\n",
    "    \"ch_best_date_of_creation\",\n",
    "    \"ch_best_score\",\n",
    "]\n",
    "\n",
    "matches = []\n",
    "for name in tqdm(df_canonical[\"company_name_norm\"], desc=\"Matching Companies House\"):\n",
    "    try:\n",
    "        m = best_match_for_name(name)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error for '{name}': {e}\")\n",
    "        m = None\n",
    "    matches.append(m)\n",
    "\n",
    "# FIX 3: Handle missing keys when normalizing\n",
    "if any(matches):\n",
    "    df_matches = pd.json_normalize(matches)\n",
    "    for col in enrich_cols:\n",
    "        if col not in df_matches.columns:\n",
    "            df_matches[col] = None\n",
    "    df_matches = df_matches[enrich_cols]\n",
    "else:\n",
    "    df_matches = pd.DataFrame(columns=enrich_cols)\n",
    "\n",
    "# Merge canonical + enriched\n",
    "df_enriched = pd.concat(\n",
    "    [df_canonical.reset_index(drop=True), df_matches.reset_index(drop=True)],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "display(df_enriched.head())\n",
    "\n",
    "# Save to output folder\n",
    "df_enriched.to_csv(OUTPUT_DIR / \"enriched_companies.csv\", index=False)\n",
    "print(f\"\\n Enriched dataset saved to: {OUTPUT_DIR / 'enriched_companies.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2850862e",
   "metadata": {},
   "source": [
    "## 6. Reporting & Visualisation\n",
    "KPIs and diagnostic charts.\n",
    "\n",
    "### What to read in the results\n",
    "- Match coverage and enriched count show how much the pipeline validated.\n",
    "- Score histogram and status distribution help spot anomalies and input drift.\n",
    "- Confidence bands (high/mid/low) guide what to accept vs send to review.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02c25c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KPIs / coverage\n",
    "kpis = {\n",
    "    \"input_rows\": int(len(df_raw)),\n",
    "    \"after_exact_dedup\": int(len(df_dedup)),\n",
    "    \"canonical_records\": int(len(df_canonical)),\n",
    "}\n",
    "\n",
    "if \"ch_best_company_number\" in df_enriched.columns:\n",
    "    kpis[\"enriched_records\"] = int(df_enriched[\"ch_best_company_number\"].notna().sum())\n",
    "    kpis[\"match_rate_pct\"] = round(100.0 * df_enriched[\"ch_best_company_number\"].notna().mean(), 2)\n",
    "else:\n",
    "    kpis[\"enriched_records\"] = 0\n",
    "    kpis[\"match_rate_pct\"] = 0.0\n",
    "\n",
    "display(pd.DataFrame([kpis]))\n",
    "\n",
    "# Status distribution\n",
    "if \"ch_best_company_status\" in df_enriched.columns:\n",
    "    status_counts = df_enriched[\"ch_best_company_status\"].fillna(\"unmatched\").value_counts().sort_values(ascending=False)\n",
    "    plt.figure()\n",
    "    status_counts.plot(kind=\"bar\", title=\"Company status distribution\")\n",
    "    plt.xlabel(\"status\"); plt.ylabel(\"count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Score histogram\n",
    "if \"ch_best_score\" in df_enriched.columns:\n",
    "    plt.figure()\n",
    "    df_enriched[\"ch_best_score\"].dropna().astype(float).plot(kind=\"hist\", bins=20, title=\"Match score histogram\")\n",
    "    plt.xlabel(\"RapidFuzz token_sort_ratio\"); plt.ylabel(\"freq\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26a2a6b",
   "metadata": {},
   "source": [
    "### Evaluation (Proxy Metrics)\n",
    "These metrics do not require labeled ground truth but still provide a quality signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bd09d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These metrics do not require ground truth and do not change your logic.\n",
    "\n",
    "eval_rows = []\n",
    "\n",
    "if \"ch_best_score\" in df_enriched.columns:\n",
    "    scores = df_enriched[\"ch_best_score\"].fillna(0).astype(float)\n",
    "    eval_rows.append({\"metric\":\"avg_score\", \"value\": float(scores.mean())})\n",
    "    eval_rows.append({\"metric\":\"p90_score\", \"value\": float(scores.quantile(0.9))})\n",
    "    eval_rows.append({\"metric\":\"high_confidence_rate_95+\", \"value\": float((scores>=95).mean())})\n",
    "    eval_rows.append({\"metric\":\"mid_confidence_rate_85_95\", \"value\": float(((scores>=85)&(scores<95)).mean())})\n",
    "    eval_rows.append({\"metric\":\"low_confidence_rate_<85\", \"value\": float((scores<85).mean())})\n",
    "\n",
    "# Ambiguity proxy: name mismatch length for high scores\n",
    "if set([\"company_name_norm\",\"ch_best_title\",\"ch_best_score\"]).issubset(df_enriched.columns):\n",
    "    def token_set(s):\n",
    "        return set(str(s).upper().split())\n",
    "    def jaccard(a,b):\n",
    "        a,b=token_set(a),token_set(b)\n",
    "        if not a and not b: return 1.0\n",
    "        return len(a&b)/max(1,len(a|b))\n",
    "    mask = df_enriched[\"ch_best_score\"].fillna(0).astype(float)>=90\n",
    "    jacc = df_enriched.loc[mask].apply(lambda r: jaccard(r[\"company_name_norm\"], r[\"ch_best_title\"]), axis=1)\n",
    "    eval_rows.append({\"metric\":\"high_score_avg_name_jaccard\", \"value\": float(jacc.mean())})\n",
    "\n",
    "eval_df = pd.DataFrame(eval_rows)\n",
    "display(eval_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddfa70c",
   "metadata": {},
   "source": [
    "## 7. Outputs\n",
    "Exports are written to `./outputs/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b4655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final outputs\n",
    "profile = profile_dataframe(df_raw)\n",
    "(OUTPUT_DIR / \"profile.csv\").write_text(\"\")\n",
    "profile.to_csv(OUTPUT_DIR / \"profile.csv\", index=False)\n",
    "\n",
    "df_enriched.to_parquet(OUTPUT_DIR / \"enriched_companies.parquet\", index=False)\n",
    "df_enriched.to_csv(OUTPUT_DIR / \"enriched_companies.csv\", index=False)\n",
    "print(\"Files written to:\", OUTPUT_DIR.resolve())\n",
    "list(OUTPUT_DIR.glob(\"*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842dea79",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
